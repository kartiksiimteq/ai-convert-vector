```markdown
# ğŸ§  CLIP Vector API (FastAPI)

A lightweight **FastAPI-based** microservice for generating text and image embeddings using **OpenAI's CLIP model**. This can be used to power AI search, recommendation, or similarity systems for content like jewelry, fashion, art, or any domain needing visual & textual understanding.


---

## ğŸš€ Features

- ğŸ”¤ Text & Image Embedding with CLIP  
- âš¡ FastAPI server for real-time vectorization  
- ğŸ§© Ready for integration with Solr or other vector databases  
- ğŸ’¡ Modular structure (`model_loader.py`, `utils.py`, etc.)

---

## ğŸ“¦ Project Structure

```
clip_vector_api/
â”œâ”€â”€ fast-api-env/           # (Optional) Virtual environment
â”œâ”€â”€ main.py                 # FastAPI app entry point
â”œâ”€â”€ model_loader.py         # CLIP model loading
â”œâ”€â”€ utils.py                # Image preprocessing & utilities
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ __pycache__/            # Python bytecode cache
```

---

## ğŸ§ª Setup & Installation

### 1. Clone the repository

```bash
git clone 
cd clip_vector_api
```

### 2. Create & activate virtual environment

```bash
python -m venv venv
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

> Note: You may need to install PyTorch manually with the correct CUDA version from https://pytorch.org if you're using a GPU.

---

## ğŸ“„ `requirements.txt`

```
fastapi
uvicorn
torch
ftfy
regex
tqdm
Pillow
git+https://github.com/openai/CLIP.git
```

---

## â–¶ï¸ Running the API Server

```bash
uvicorn main:app --reload
```

The API will be available at:  
ğŸ“ `http://127.0.0.1:8000`

---

## ğŸ“« API Usage (Sample)

### ğŸ”¡ `/vectorize/text` â€“ Generate text embedding

```http
POST /vectorize/text
Content-Type: application/json

{
  "text": "Elegant diamond pendant in rose gold"
}
```

### ğŸ–¼ï¸ `/vectorize/image` â€“ Generate image embedding

Send an image via `multipart/form-data` using a form upload.

---

## ğŸ’¡ Notes

- You can use the generated embeddings directly with vector search engines like **Solr**, **Weaviate**, **FAISS**, or **Pinecone**.
- The embeddings are 512-dimensional vectors from the ViT-B/32 CLIP model.

---
